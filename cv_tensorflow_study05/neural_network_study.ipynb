{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('week09_venv': venv)"
  },
  "interpreter": {
   "hash": "e5245979aa73c3b83432dc2a37ab2e9505e3421d94f01464585bd10ff7b289bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "neural_network_study.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.5438,  0.7618,  2.7988, -0.4246, -0.8549,  0.3536,  1.0075,  0.7067,\n",
       "          -0.7033,  0.8669],\n",
       "         [ 0.6087, -0.3894, -0.4958, -0.6928,  0.5338, -0.5139, -1.4149, -1.2338,\n",
       "           0.4246,  0.0633],\n",
       "         [-0.3636,  0.0657,  1.1368,  1.9268, -0.4148,  0.4798,  0.5540,  1.9926,\n",
       "          -1.2777, -2.6687],\n",
       "         [-0.9780, -1.1629, -1.9202, -1.6580,  0.3696, -0.4899,  0.2291, -1.0204,\n",
       "           1.8573, -0.0234],\n",
       "         [-0.3130, -1.0038, -1.2396,  0.8708, -0.9561, -1.2947, -0.9094, -0.5862,\n",
       "          -1.3043, -0.3365],\n",
       "         [-0.0107,  0.0763,  0.6213,  1.2972, -1.1637,  1.2142,  1.5342,  0.1677,\n",
       "          -0.5744,  0.3541],\n",
       "         [-2.3239, -0.8488, -0.4629, -0.9441,  1.0011, -0.4377,  2.1793, -2.0460,\n",
       "          -0.3604, -0.3297],\n",
       "         [ 0.8936,  0.3977,  1.2376,  0.6749,  1.0424,  0.0615, -0.4844, -1.7592,\n",
       "           1.0900,  0.6621],\n",
       "         [ 0.1590,  0.2590, -0.4724, -0.0984,  0.6236,  0.6847,  0.4409, -0.2471,\n",
       "          -0.3004,  0.6149],\n",
       "         [-0.1272,  0.7851,  0.0068,  0.6309,  0.6392, -0.2362, -1.1908,  0.2406,\n",
       "          -0.4553, -0.1083]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([ [1.], [1.], [0.], [1.], [0.], [1.], [0.], [1.], [1.], [0.] ])\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_in, n_h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_h, n_out),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " :  0.0160058606415987\n",
      "epoch :  2034 loss :  0.01599273458123207\n",
      "epoch :  2035 loss :  0.015979718416929245\n",
      "epoch :  2036 loss :  0.015967048704624176\n",
      "epoch :  2037 loss :  0.01595389097929001\n",
      "epoch :  2038 loss :  0.015941187739372253\n",
      "epoch :  2039 loss :  0.01592833921313286\n",
      "epoch :  2040 loss :  0.01591542549431324\n",
      "epoch :  2041 loss :  0.015902459621429443\n",
      "epoch :  2042 loss :  0.015889693051576614\n",
      "epoch :  2043 loss :  0.015877094119787216\n",
      "epoch :  2044 loss :  0.015863917768001556\n",
      "epoch :  2045 loss :  0.015851283445954323\n",
      "epoch :  2046 loss :  0.015838731080293655\n",
      "epoch :  2047 loss :  0.015825703740119934\n",
      "epoch :  2048 loss :  0.01581278070807457\n",
      "epoch :  2049 loss :  0.015800392255187035\n",
      "epoch :  2050 loss :  0.01578766480088234\n",
      "epoch :  2051 loss :  0.015774598345160484\n",
      "epoch :  2052 loss :  0.01576211303472519\n",
      "epoch :  2053 loss :  0.01574987918138504\n",
      "epoch :  2054 loss :  0.01573674939572811\n",
      "epoch :  2055 loss :  0.01572389528155327\n",
      "epoch :  2056 loss :  0.015711946412920952\n",
      "epoch :  2057 loss :  0.015699021518230438\n",
      "epoch :  2058 loss :  0.015685971826314926\n",
      "epoch :  2059 loss :  0.015673983842134476\n",
      "epoch :  2060 loss :  0.015661420300602913\n",
      "epoch :  2061 loss :  0.01564839482307434\n",
      "epoch :  2062 loss :  0.015635820105671883\n",
      "epoch :  2063 loss :  0.015623928979039192\n",
      "epoch :  2064 loss :  0.015610967762768269\n",
      "epoch :  2065 loss :  0.01559821330010891\n",
      "epoch :  2066 loss :  0.015586396679282188\n",
      "epoch :  2067 loss :  0.015573667362332344\n",
      "epoch :  2068 loss :  0.015560874715447426\n",
      "epoch :  2069 loss :  0.015548596158623695\n",
      "epoch :  2070 loss :  0.015536462888121605\n",
      "epoch :  2071 loss :  0.015523741953074932\n",
      "epoch :  2072 loss :  0.015511035919189453\n",
      "epoch :  2073 loss :  0.01549945306032896\n",
      "epoch :  2074 loss :  0.015486766584217548\n",
      "epoch :  2075 loss :  0.015473933890461922\n",
      "epoch :  2076 loss :  0.015462162904441357\n",
      "epoch :  2077 loss :  0.015449839644134045\n",
      "epoch :  2078 loss :  0.015437101013958454\n",
      "epoch :  2079 loss :  0.01542488019913435\n",
      "epoch :  2080 loss :  0.015413030982017517\n",
      "epoch :  2081 loss :  0.015400390140712261\n",
      "epoch :  2082 loss :  0.01538791786879301\n",
      "epoch :  2083 loss :  0.015376059338450432\n",
      "epoch :  2084 loss :  0.015363752841949463\n",
      "epoch :  2085 loss :  0.015351193957030773\n",
      "epoch :  2086 loss :  0.015339297242462635\n",
      "epoch :  2087 loss :  0.01532723754644394\n",
      "epoch :  2088 loss :  0.015314852818846703\n",
      "epoch :  2089 loss :  0.015302675776183605\n",
      "epoch :  2090 loss :  0.015290670096874237\n",
      "epoch :  2091 loss :  0.015278498642146587\n",
      "epoch :  2092 loss :  0.015266175381839275\n",
      "epoch :  2093 loss :  0.015254221856594086\n",
      "epoch :  2094 loss :  0.015242273919284344\n",
      "epoch :  2095 loss :  0.015229834243655205\n",
      "epoch :  2096 loss :  0.015218059532344341\n",
      "epoch :  2097 loss :  0.015205864794552326\n",
      "epoch :  2098 loss :  0.015193772502243519\n",
      "epoch :  2099 loss :  0.015181842260062695\n",
      "epoch :  2100 loss :  0.015169793739914894\n",
      "epoch :  2101 loss :  0.015157761052250862\n",
      "epoch :  2102 loss :  0.015145723707973957\n",
      "epoch :  2103 loss :  0.015133984386920929\n",
      "epoch :  2104 loss :  0.015121770091354847\n",
      "epoch :  2105 loss :  0.015109708532691002\n",
      "epoch :  2106 loss :  0.015098285861313343\n",
      "epoch :  2107 loss :  0.01508593000471592\n",
      "epoch :  2108 loss :  0.015074165537953377\n",
      "epoch :  2109 loss :  0.015062469057738781\n",
      "epoch :  2110 loss :  0.015050400979816914\n",
      "epoch :  2111 loss :  0.015038448385894299\n",
      "epoch :  2112 loss :  0.015026720240712166\n",
      "epoch :  2113 loss :  0.01501498930156231\n",
      "epoch :  2114 loss :  0.015002856031060219\n",
      "epoch :  2115 loss :  0.01499113254249096\n",
      "epoch :  2116 loss :  0.014979678206145763\n",
      "epoch :  2117 loss :  0.014967565424740314\n",
      "epoch :  2118 loss :  0.014955560676753521\n",
      "epoch :  2119 loss :  0.01494425069540739\n",
      "epoch :  2120 loss :  0.014932428486645222\n",
      "epoch :  2121 loss :  0.014920299872756004\n",
      "epoch :  2122 loss :  0.01490896474570036\n",
      "epoch :  2123 loss :  0.014897438697516918\n",
      "epoch :  2124 loss :  0.014885330572724342\n",
      "epoch :  2125 loss :  0.014873573556542397\n",
      "epoch :  2126 loss :  0.014862490817904472\n",
      "epoch :  2127 loss :  0.014850479550659657\n",
      "epoch :  2128 loss :  0.0148384440690279\n",
      "epoch :  2129 loss :  0.014827551320195198\n",
      "epoch :  2130 loss :  0.014815708622336388\n",
      "epoch :  2131 loss :  0.014803729951381683\n",
      "epoch :  2132 loss :  0.014792317524552345\n",
      "epoch :  2133 loss :  0.014781029894948006\n",
      "epoch :  2134 loss :  0.014769138768315315\n",
      "epoch :  2135 loss :  0.014757402241230011\n",
      "epoch :  2136 loss :  0.014746447093784809\n",
      "epoch :  2137 loss :  0.014734745025634766\n",
      "epoch :  2138 loss :  0.014722837135195732\n",
      "epoch :  2139 loss :  0.01471175067126751\n",
      "epoch :  2140 loss :  0.014700201340019703\n",
      "epoch :  2141 loss :  0.014688529074192047\n",
      "epoch :  2142 loss :  0.014677053317427635\n",
      "epoch :  2143 loss :  0.01466607116162777\n",
      "epoch :  2144 loss :  0.014654284343123436\n",
      "epoch :  2145 loss :  0.014642667956650257\n",
      "epoch :  2146 loss :  0.014631727710366249\n",
      "epoch :  2147 loss :  0.014620150439441204\n",
      "epoch :  2148 loss :  0.014608433470129967\n",
      "epoch :  2149 loss :  0.014597472734749317\n",
      "epoch :  2150 loss :  0.01458617765456438\n",
      "epoch :  2151 loss :  0.014574448578059673\n",
      "epoch :  2152 loss :  0.014563304372131824\n",
      "epoch :  2153 loss :  0.014552022330462933\n",
      "epoch :  2154 loss :  0.014540580101311207\n",
      "epoch :  2155 loss :  0.014529290609061718\n",
      "epoch :  2156 loss :  0.01451810635626316\n",
      "epoch :  2157 loss :  0.014506904408335686\n",
      "epoch :  2158 loss :  0.014495405368506908\n",
      "epoch :  2159 loss :  0.014484475366771221\n",
      "epoch :  2160 loss :  0.014473089948296547\n",
      "epoch :  2161 loss :  0.014461773447692394\n",
      "epoch :  2162 loss :  0.014450764283537865\n",
      "epoch :  2163 loss :  0.01443939097225666\n",
      "epoch :  2164 loss :  0.014428332448005676\n",
      "epoch :  2165 loss :  0.014417095109820366\n",
      "epoch :  2166 loss :  0.014405992813408375\n",
      "epoch :  2167 loss :  0.01439468003809452\n",
      "epoch :  2168 loss :  0.014383547008037567\n",
      "epoch :  2169 loss :  0.014372709207236767\n",
      "epoch :  2170 loss :  0.014361308887600899\n",
      "epoch :  2171 loss :  0.014350198209285736\n",
      "epoch :  2172 loss :  0.014339521527290344\n",
      "epoch :  2173 loss :  0.01432819664478302\n",
      "epoch :  2174 loss :  0.014316918328404427\n",
      "epoch :  2175 loss :  0.014306101016700268\n",
      "epoch :  2176 loss :  0.014295177534222603\n",
      "epoch :  2177 loss :  0.01428375393152237\n",
      "epoch :  2178 loss :  0.014272963628172874\n",
      "epoch :  2179 loss :  0.014262223616242409\n",
      "epoch :  2180 loss :  0.014250896871089935\n",
      "epoch :  2181 loss :  0.014239868149161339\n",
      "epoch :  2182 loss :  0.014229397289454937\n",
      "epoch :  2183 loss :  0.014218131080269814\n",
      "epoch :  2184 loss :  0.014206944033503532\n",
      "epoch :  2185 loss :  0.014196515083312988\n",
      "epoch :  2186 loss :  0.014185445383191109\n",
      "epoch :  2187 loss :  0.014174570329487324\n",
      "epoch :  2188 loss :  0.01416372787207365\n",
      "epoch :  2189 loss :  0.01415317039936781\n",
      "epoch :  2190 loss :  0.014141951687633991\n",
      "epoch :  2191 loss :  0.014130826108157635\n",
      "epoch :  2192 loss :  0.014120785519480705\n",
      "epoch :  2193 loss :  0.01410963386297226\n",
      "epoch :  2194 loss :  0.01409839279949665\n",
      "epoch :  2195 loss :  0.014088226482272148\n",
      "epoch :  2196 loss :  0.014077556319534779\n",
      "epoch :  2197 loss :  0.014066427946090698\n",
      "epoch :  2198 loss :  0.014055661857128143\n",
      "epoch :  2199 loss :  0.014045359566807747\n",
      "epoch :  2200 loss :  0.014034269377589226\n",
      "epoch :  2201 loss :  0.01402330119162798\n",
      "epoch :  2202 loss :  0.014013020321726799\n",
      "epoch :  2203 loss :  0.014002556912600994\n",
      "epoch :  2204 loss :  0.013991331681609154\n",
      "epoch :  2205 loss :  0.013981061987578869\n",
      "epoch :  2206 loss :  0.01397048681974411\n",
      "epoch :  2207 loss :  0.013959434814751148\n",
      "epoch :  2208 loss :  0.013948949053883553\n",
      "epoch :  2209 loss :  0.013938471674919128\n",
      "epoch :  2210 loss :  0.01392770279198885\n",
      "epoch :  2211 loss :  0.013917309232056141\n",
      "epoch :  2212 loss :  0.013906734995543957\n",
      "epoch :  2213 loss :  0.013896167278289795\n",
      "epoch :  2214 loss :  0.01388537883758545\n",
      "epoch :  2215 loss :  0.013875087723135948\n",
      "epoch :  2216 loss :  0.013864494860172272\n",
      "epoch :  2217 loss :  0.01385380607098341\n",
      "epoch :  2218 loss :  0.01384351123124361\n",
      "epoch :  2219 loss :  0.01383311115205288\n",
      "epoch :  2220 loss :  0.013822568580508232\n",
      "epoch :  2221 loss :  0.013811987824738026\n",
      "epoch :  2222 loss :  0.013801584020256996\n",
      "epoch :  2223 loss :  0.013791011646389961\n",
      "epoch :  2224 loss :  0.013780489563941956\n",
      "epoch :  2225 loss :  0.01377025991678238\n",
      "epoch :  2226 loss :  0.013759732246398926\n",
      "epoch :  2227 loss :  0.01374940574169159\n",
      "epoch :  2228 loss :  0.013739260844886303\n",
      "epoch :  2229 loss :  0.013728599064052105\n",
      "epoch :  2230 loss :  0.013718120753765106\n",
      "epoch :  2231 loss :  0.01370809506624937\n",
      "epoch :  2232 loss :  0.013697574846446514\n",
      "epoch :  2233 loss :  0.013686902821063995\n",
      "epoch :  2234 loss :  0.013676921837031841\n",
      "epoch :  2235 loss :  0.013666840270161629\n",
      "epoch :  2236 loss :  0.013656173832714558\n",
      "epoch :  2237 loss :  0.013645963743329048\n",
      "epoch :  2238 loss :  0.013635987415909767\n",
      "epoch :  2239 loss :  0.013625356368720531\n",
      "epoch :  2240 loss :  0.01361495815217495\n",
      "epoch :  2241 loss :  0.013605115935206413\n",
      "epoch :  2242 loss :  0.013594667427241802\n",
      "epoch :  2243 loss :  0.013584311120212078\n",
      "epoch :  2244 loss :  0.013574379496276379\n",
      "epoch :  2245 loss :  0.013564268127083778\n",
      "epoch :  2246 loss :  0.013553736731410027\n",
      "epoch :  2247 loss :  0.01354343630373478\n",
      "epoch :  2248 loss :  0.01353377290070057\n",
      "epoch :  2249 loss :  0.013523273169994354\n",
      "epoch :  2250 loss :  0.01351284421980381\n",
      "epoch :  2251 loss :  0.01350364089012146\n",
      "epoch :  2252 loss :  0.01349315233528614\n",
      "epoch :  2253 loss :  0.013482680544257164\n",
      "epoch :  2254 loss :  0.013472804799675941\n",
      "epoch :  2255 loss :  0.013462863862514496\n",
      "epoch :  2256 loss :  0.013452431187033653\n",
      "epoch :  2257 loss :  0.013442267663776875\n",
      "epoch :  2258 loss :  0.013432687148451805\n",
      "epoch :  2259 loss :  0.013422605581581593\n",
      "epoch :  2260 loss :  0.013412143103778362\n",
      "epoch :  2261 loss :  0.013402539305388927\n",
      "epoch :  2262 loss :  0.013392455875873566\n",
      "epoch :  2263 loss :  0.013382052071392536\n",
      "epoch :  2264 loss :  0.013372333720326424\n",
      "epoch :  2265 loss :  0.013362457975745201\n",
      "epoch :  2266 loss :  0.013352490961551666\n",
      "epoch :  2267 loss :  0.013342509977519512\n",
      "epoch :  2268 loss :  0.013332572765648365\n",
      "epoch :  2269 loss :  0.013322519138455391\n",
      "epoch :  2270 loss :  0.01331255305558443\n",
      "epoch :  2271 loss :  0.013302679173648357\n",
      "epoch :  2272 loss :  0.013292854651808739\n",
      "epoch :  2273 loss :  0.01328268926590681\n",
      "epoch :  2274 loss :  0.013273278251290321\n",
      "epoch :  2275 loss :  0.013263337314128876\n",
      "epoch :  2276 loss :  0.013253262266516685\n",
      "epoch :  2277 loss :  0.013243491761386395\n",
      "epoch :  2278 loss :  0.01323358528316021\n",
      "epoch :  2279 loss :  0.01322377473115921\n",
      "epoch :  2280 loss :  0.01321386732161045\n",
      "epoch :  2281 loss :  0.01320407260209322\n",
      "epoch :  2282 loss :  0.013194417580962181\n",
      "epoch :  2283 loss :  0.013184579089283943\n",
      "epoch :  2284 loss :  0.013174953870475292\n",
      "epoch :  2285 loss :  0.013164831325411797\n",
      "epoch :  2286 loss :  0.013155145570635796\n",
      "epoch :  2287 loss :  0.01314575970172882\n",
      "epoch :  2288 loss :  0.013135632500052452\n",
      "epoch :  2289 loss :  0.01312575675547123\n",
      "epoch :  2290 loss :  0.013116782531142235\n",
      "epoch :  2291 loss :  0.013106733560562134\n",
      "epoch :  2292 loss :  0.01309674046933651\n",
      "epoch :  2293 loss :  0.013087397441267967\n",
      "epoch :  2294 loss :  0.01307767815887928\n",
      "epoch :  2295 loss :  0.013067694380879402\n",
      "epoch :  2296 loss :  0.013058026321232319\n",
      "epoch :  2297 loss :  0.013048820197582245\n",
      "epoch :  2298 loss :  0.013039092533290386\n",
      "epoch :  2299 loss :  0.01302916370332241\n",
      "epoch :  2300 loss :  0.013020120561122894\n",
      "epoch :  2301 loss :  0.01301017589867115\n",
      "epoch :  2302 loss :  0.013000277802348137\n",
      "epoch :  2303 loss :  0.012990996241569519\n",
      "epoch :  2304 loss :  0.012981469742953777\n",
      "epoch :  2305 loss :  0.012971711345016956\n",
      "epoch :  2306 loss :  0.012962309643626213\n",
      "epoch :  2307 loss :  0.012953022494912148\n",
      "epoch :  2308 loss :  0.012943217530846596\n",
      "epoch :  2309 loss :  0.012933442369103432\n",
      "epoch :  2310 loss :  0.012924456968903542\n",
      "epoch :  2311 loss :  0.012914726510643959\n",
      "epoch :  2312 loss :  0.012904919683933258\n",
      "epoch :  2313 loss :  0.012895974330604076\n",
      "epoch :  2314 loss :  0.012886567041277885\n",
      "epoch :  2315 loss :  0.012876707129180431\n",
      "epoch :  2316 loss :  0.012867292389273643\n",
      "epoch :  2317 loss :  0.012858102098107338\n",
      "epoch :  2318 loss :  0.012848429381847382\n",
      "epoch :  2319 loss :  0.012838865630328655\n",
      "epoch :  2320 loss :  0.012829688377678394\n",
      "epoch :  2321 loss :  0.01282047200948\n",
      "epoch :  2322 loss :  0.012810801155865192\n",
      "epoch :  2323 loss :  0.012801671400666237\n",
      "epoch :  2324 loss :  0.012792234309017658\n",
      "epoch :  2325 loss :  0.012782732024788857\n",
      "epoch :  2326 loss :  0.01277353148907423\n",
      "epoch :  2327 loss :  0.01276404969394207\n",
      "epoch :  2328 loss :  0.012754607014358044\n",
      "epoch :  2329 loss :  0.01274570357054472\n",
      "epoch :  2330 loss :  0.012736210599541664\n",
      "epoch :  2331 loss :  0.012727022171020508\n",
      "epoch :  2332 loss :  0.012717572040855885\n",
      "epoch :  2333 loss :  0.012708346359431744\n",
      "epoch :  2334 loss :  0.012699050828814507\n",
      "epoch :  2335 loss :  0.012689648196101189\n",
      "epoch :  2336 loss :  0.012680599465966225\n",
      "epoch :  2337 loss :  0.01267146784812212\n",
      "epoch :  2338 loss :  0.012662112712860107\n",
      "epoch :  2339 loss :  0.012653062120079994\n",
      "epoch :  2340 loss :  0.012643647380173206\n",
      "epoch :  2341 loss :  0.012634262442588806\n",
      "epoch :  2342 loss :  0.012625331990420818\n",
      "epoch :  2343 loss :  0.012616108171641827\n",
      "epoch :  2344 loss :  0.01260666735470295\n",
      "epoch :  2345 loss :  0.012597988359630108\n",
      "epoch :  2346 loss :  0.012588804587721825\n",
      "epoch :  2347 loss :  0.012579415924847126\n",
      "epoch :  2348 loss :  0.012570211663842201\n",
      "epoch :  2349 loss :  0.012561516836285591\n",
      "epoch :  2350 loss :  0.012552049942314625\n",
      "epoch :  2351 loss :  0.012542704120278358\n",
      "epoch :  2352 loss :  0.01253425795584917\n",
      "epoch :  2353 loss :  0.012525087222456932\n",
      "epoch :  2354 loss :  0.012515561655163765\n",
      "epoch :  2355 loss :  0.01250679511576891\n",
      "epoch :  2356 loss :  0.0124978581443429\n",
      "epoch :  2357 loss :  0.012488426640629768\n",
      "epoch :  2358 loss :  0.012479348108172417\n",
      "epoch :  2359 loss :  0.012470816262066364\n",
      "epoch :  2360 loss :  0.012461584992706776\n",
      "epoch :  2361 loss :  0.01245238445699215\n",
      "epoch :  2362 loss :  0.012443833984434605\n",
      "epoch :  2363 loss :  0.012434594333171844\n",
      "epoch :  2364 loss :  0.012425401248037815\n",
      "epoch :  2365 loss :  0.012416675686836243\n",
      "epoch :  2366 loss :  0.012407719157636166\n",
      "epoch :  2367 loss :  0.012398525141179562\n",
      "epoch :  2368 loss :  0.012389854528009892\n",
      "epoch :  2369 loss :  0.012381201609969139\n",
      "epoch :  2370 loss :  0.012371983379125595\n",
      "epoch :  2371 loss :  0.012362893670797348\n",
      "epoch :  2372 loss :  0.012354296632111073\n",
      "epoch :  2373 loss :  0.012345312163233757\n",
      "epoch :  2374 loss :  0.012336104176938534\n",
      "epoch :  2375 loss :  0.012327478267252445\n",
      "epoch :  2376 loss :  0.012319022789597511\n",
      "epoch :  2377 loss :  0.012309695594012737\n",
      "epoch :  2378 loss :  0.012301050126552582\n",
      "epoch :  2379 loss :  0.012292124330997467\n",
      "epoch :  2380 loss :  0.012283192947506905\n",
      "epoch :  2381 loss :  0.012274453416466713\n",
      "epoch :  2382 loss :  0.012265573255717754\n",
      "epoch :  2383 loss :  0.012256830930709839\n",
      "epoch :  2384 loss :  0.012248319573700428\n",
      "epoch :  2385 loss :  0.012239417992532253\n",
      "epoch :  2386 loss :  0.012230532243847847\n",
      "epoch :  2387 loss :  0.012221753597259521\n",
      "epoch :  2388 loss :  0.012213138863444328\n",
      "epoch :  2389 loss :  0.012204143218696117\n",
      "epoch :  2390 loss :  0.01219537016004324\n",
      "epoch :  2391 loss :  0.01218702457845211\n",
      "epoch :  2392 loss :  0.012178215198218822\n",
      "epoch :  2393 loss :  0.01216940488666296\n",
      "epoch :  2394 loss :  0.01216091401875019\n",
      "epoch :  2395 loss :  0.012152043171226978\n",
      "epoch :  2396 loss :  0.012143208645284176\n",
      "epoch :  2397 loss :  0.012134736403822899\n",
      "epoch :  2398 loss :  0.01212609838694334\n",
      "epoch :  2399 loss :  0.012117347680032253\n",
      "epoch :  2400 loss :  0.012108728289604187\n",
      "epoch :  2401 loss :  0.01210032682865858\n",
      "epoch :  2402 loss :  0.012091435492038727\n",
      "epoch :  2403 loss :  0.012082620523869991\n",
      "epoch :  2404 loss :  0.012074491009116173\n",
      "epoch :  2405 loss :  0.01206560991704464\n",
      "epoch :  2406 loss :  0.012056849896907806\n",
      "epoch :  2407 loss :  0.0120488116517663\n",
      "epoch :  2408 loss :  0.012040168978273869\n",
      "epoch :  2409 loss :  0.012031233869493008\n",
      "epoch :  2410 loss :  0.012022821232676506\n",
      "epoch :  2411 loss :  0.012014542706310749\n",
      "epoch :  2412 loss :  0.012005635537207127\n",
      "epoch :  2413 loss :  0.01199694350361824\n",
      "epoch :  2414 loss :  0.01198897697031498\n",
      "epoch :  2415 loss :  0.011980412527918816\n",
      "epoch :  2416 loss :  0.011971533298492432\n",
      "epoch :  2417 loss :  0.011963444761931896\n",
      "epoch :  2418 loss :  0.011954830028116703\n",
      "epoch :  2419 loss :  0.011946055106818676\n",
      "epoch :  2420 loss :  0.011937810108065605\n",
      "epoch :  2421 loss :  0.01192946545779705\n",
      "epoch :  2422 loss :  0.011920739896595478\n",
      "epoch :  2423 loss :  0.011912515386939049\n",
      "epoch :  2424 loss :  0.01190425269305706\n",
      "epoch :  2425 loss :  0.011895628646016121\n",
      "epoch :  2426 loss :  0.011887148953974247\n",
      "epoch :  2427 loss :  0.011878876015543938\n",
      "epoch :  2428 loss :  0.011870457790791988\n",
      "epoch :  2429 loss :  0.01186186634004116\n",
      "epoch :  2430 loss :  0.011853773146867752\n",
      "epoch :  2431 loss :  0.011845642700791359\n",
      "epoch :  2432 loss :  0.011836934834718704\n",
      "epoch :  2433 loss :  0.01182876992970705\n",
      "epoch :  2434 loss :  0.01182033121585846\n",
      "epoch :  2435 loss :  0.011811907403171062\n",
      "epoch :  2436 loss :  0.011803673580288887\n",
      "epoch :  2437 loss :  0.011795279569923878\n",
      "epoch :  2438 loss :  0.011787250638008118\n",
      "epoch :  2439 loss :  0.011778803542256355\n",
      "epoch :  2440 loss :  0.011770518496632576\n",
      "epoch :  2441 loss :  0.011762123554944992\n",
      "epoch :  2442 loss :  0.01175387017428875\n",
      "epoch :  2443 loss :  0.011745715513825417\n",
      "epoch :  2444 loss :  0.011737223714590073\n",
      "epoch :  2445 loss :  0.011729122139513493\n",
      "epoch :  2446 loss :  0.011721319518983364\n",
      "epoch :  2447 loss :  0.01171270664781332\n",
      "epoch :  2448 loss :  0.011704334989190102\n",
      "epoch :  2449 loss :  0.011696534231305122\n",
      "epoch :  2450 loss :  0.01168810110539198\n",
      "epoch :  2451 loss :  0.011679607443511486\n",
      "epoch :  2452 loss :  0.011671723797917366\n",
      "epoch :  2453 loss :  0.011663754470646381\n",
      "epoch :  2454 loss :  0.011655254289507866\n",
      "epoch :  2455 loss :  0.011647322215139866\n",
      "epoch :  2456 loss :  0.011639267206192017\n",
      "epoch :  2457 loss :  0.011630801483988762\n",
      "epoch :  2458 loss :  0.01162264496088028\n",
      "epoch :  2459 loss :  0.01161489263176918\n",
      "epoch :  2460 loss :  0.01160641573369503\n",
      "epoch :  2461 loss :  0.011598343029618263\n",
      "epoch :  2462 loss :  0.011590620502829552\n",
      "epoch :  2463 loss :  0.011582350358366966\n",
      "epoch :  2464 loss :  0.01157398335635662\n",
      "epoch :  2465 loss :  0.011566107161343098\n",
      "epoch :  2466 loss :  0.01155815925449133\n",
      "epoch :  2467 loss :  0.011549802497029305\n",
      "epoch :  2468 loss :  0.011541738174855709\n",
      "epoch :  2469 loss :  0.0115343127399683\n",
      "epoch :  2470 loss :  0.011525902897119522\n",
      "epoch :  2471 loss :  0.011517727747559547\n",
      "epoch :  2472 loss :  0.011510002426803112\n",
      "epoch :  2473 loss :  0.011501861736178398\n",
      "epoch :  2474 loss :  0.011493591591715813\n",
      "epoch :  2475 loss :  0.011485874652862549\n",
      "epoch :  2476 loss :  0.011478006839752197\n",
      "epoch :  2477 loss :  0.011469880118966103\n",
      "epoch :  2478 loss :  0.01146195363253355\n",
      "epoch :  2479 loss :  0.011454056017100811\n",
      "epoch :  2480 loss :  0.011445935815572739\n",
      "epoch :  2481 loss :  0.011438002809882164\n",
      "epoch :  2482 loss :  0.011430088430643082\n",
      "epoch :  2483 loss :  0.011422181501984596\n",
      "epoch :  2484 loss :  0.011414330452680588\n",
      "epoch :  2485 loss :  0.011406485922634602\n",
      "epoch :  2486 loss :  0.011398496106266975\n",
      "epoch :  2487 loss :  0.011390497907996178\n",
      "epoch :  2488 loss :  0.011382820084691048\n",
      "epoch :  2489 loss :  0.011374728754162788\n",
      "epoch :  2490 loss :  0.011366743594408035\n",
      "epoch :  2491 loss :  0.011359324678778648\n",
      "epoch :  2492 loss :  0.011351222172379494\n",
      "epoch :  2493 loss :  0.011343470774590969\n",
      "epoch :  2494 loss :  0.011335759423673153\n",
      "epoch :  2495 loss :  0.011327704414725304\n",
      "epoch :  2496 loss :  0.011319865472614765\n",
      "epoch :  2497 loss :  0.011312202550470829\n",
      "epoch :  2498 loss :  0.011304272338747978\n",
      "epoch :  2499 loss :  0.011296342127025127\n",
      "epoch :  2500 loss :  0.011288878507912159\n"
     ]
    }
   ],
   "source": [
    "for i in range(2500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "\n",
    "    print('epoch : ', (i + 1), \"loss : \", loss.item())\n",
    "    loss.backward() # 역전파 적용\n",
    "\n",
    "    optimizer.step() # 모든 파라미터 최신화\n",
    "\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "source": [
    "## torch.nn.module을 사용해서 많은 간단한 레이어를 복잡한 뉴럴 네트워크로 만들 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_class(nn.Module):\n",
    "    def __init__(self, n_in, n_h, n_out):\n",
    "        super(Custom_class, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_in, n_h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_h, n_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nn = Custom_class(n_in, n_h, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.003916525281965733\n",
      "epoch :  2042 loss :  0.003915096633136272\n",
      "epoch :  2043 loss :  0.00391352828592062\n",
      "epoch :  2044 loss :  0.003912078682333231\n",
      "epoch :  2045 loss :  0.0039106192998588085\n",
      "epoch :  2046 loss :  0.00390907097607851\n",
      "epoch :  2047 loss :  0.0039076171815395355\n",
      "epoch :  2048 loss :  0.0039061219431459904\n",
      "epoch :  2049 loss :  0.003904605284333229\n",
      "epoch :  2050 loss :  0.003903129370883107\n",
      "epoch :  2051 loss :  0.0039016064256429672\n",
      "epoch :  2052 loss :  0.003900171723216772\n",
      "epoch :  2053 loss :  0.0038986620493233204\n",
      "epoch :  2054 loss :  0.003897163551300764\n",
      "epoch :  2055 loss :  0.0038957339711487293\n",
      "epoch :  2056 loss :  0.0038941637612879276\n",
      "epoch :  2057 loss :  0.0038927230052649975\n",
      "epoch :  2058 loss :  0.0038912673480808735\n",
      "epoch :  2059 loss :  0.0038897048216313124\n",
      "epoch :  2060 loss :  0.0038883157540112734\n",
      "epoch :  2061 loss :  0.0038868163246661425\n",
      "epoch :  2062 loss :  0.0038853385485708714\n",
      "epoch :  2063 loss :  0.0038838591426610947\n",
      "epoch :  2064 loss :  0.003882360178977251\n",
      "epoch :  2065 loss :  0.0038809124380350113\n",
      "epoch :  2066 loss :  0.003879392985254526\n",
      "epoch :  2067 loss :  0.003877973882481456\n",
      "epoch :  2068 loss :  0.003876536386087537\n",
      "epoch :  2069 loss :  0.003874975722283125\n",
      "epoch :  2070 loss :  0.0038735598791390657\n",
      "epoch :  2071 loss :  0.0038720828015357256\n",
      "epoch :  2072 loss :  0.003870548214763403\n",
      "epoch :  2073 loss :  0.0038691633380949497\n",
      "epoch :  2074 loss :  0.0038676667027175426\n",
      "epoch :  2075 loss :  0.003866198705509305\n",
      "epoch :  2076 loss :  0.003864747006446123\n",
      "epoch :  2077 loss :  0.00386325316503644\n",
      "epoch :  2078 loss :  0.0038618301041424274\n",
      "epoch :  2079 loss :  0.003860295517370105\n",
      "epoch :  2080 loss :  0.0038588973693549633\n",
      "epoch :  2081 loss :  0.003857497125864029\n",
      "epoch :  2082 loss :  0.0038559220265597105\n",
      "epoch :  2083 loss :  0.0038545536808669567\n",
      "epoch :  2084 loss :  0.0038530558813363314\n",
      "epoch :  2085 loss :  0.0038515522610396147\n",
      "epoch :  2086 loss :  0.0038501559756696224\n",
      "epoch :  2087 loss :  0.0038486667908728123\n",
      "epoch :  2088 loss :  0.0038472446613013744\n",
      "epoch :  2089 loss :  0.003845795290544629\n",
      "epoch :  2090 loss :  0.0038443126250058413\n",
      "epoch :  2091 loss :  0.003842889564111829\n",
      "epoch :  2092 loss :  0.003841368481516838\n",
      "epoch :  2093 loss :  0.003840011777356267\n",
      "epoch :  2094 loss :  0.0038385253865271807\n",
      "epoch :  2095 loss :  0.003837042022496462\n",
      "epoch :  2096 loss :  0.0038357090670615435\n",
      "epoch :  2097 loss :  0.003834181698039174\n",
      "epoch :  2098 loss :  0.0038327244110405445\n",
      "epoch :  2099 loss :  0.0038313488475978374\n",
      "epoch :  2100 loss :  0.0038298300933092833\n",
      "epoch :  2101 loss :  0.0038284037727862597\n",
      "epoch :  2102 loss :  0.003826997010037303\n",
      "epoch :  2103 loss :  0.003825535997748375\n",
      "epoch :  2104 loss :  0.0038241022266447544\n",
      "epoch :  2105 loss :  0.003822615835815668\n",
      "epoch :  2106 loss :  0.003821229562163353\n",
      "epoch :  2107 loss :  0.0038197743706405163\n",
      "epoch :  2108 loss :  0.0038182989228516817\n",
      "epoch :  2109 loss :  0.0038169673644006252\n",
      "epoch :  2110 loss :  0.0038154590874910355\n",
      "epoch :  2111 loss :  0.0038140483666211367\n",
      "epoch :  2112 loss :  0.0038126676809042692\n",
      "epoch :  2113 loss :  0.0038111447356641293\n",
      "epoch :  2114 loss :  0.0038097489159554243\n",
      "epoch :  2115 loss :  0.003808302339166403\n",
      "epoch :  2116 loss :  0.0038069146685302258\n",
      "epoch :  2117 loss :  0.0038054913748055696\n",
      "epoch :  2118 loss :  0.0038040168583393097\n",
      "epoch :  2119 loss :  0.003802637103945017\n",
      "epoch :  2120 loss :  0.003801175858825445\n",
      "epoch :  2121 loss :  0.0037997285835444927\n",
      "epoch :  2122 loss :  0.0037983604706823826\n",
      "epoch :  2123 loss :  0.0037969208788126707\n",
      "epoch :  2124 loss :  0.0037954910658299923\n",
      "epoch :  2125 loss :  0.00379410688765347\n",
      "epoch :  2126 loss :  0.003792633768171072\n",
      "epoch :  2127 loss :  0.003791237948462367\n",
      "epoch :  2128 loss :  0.00378982606343925\n",
      "epoch :  2129 loss :  0.003788446309044957\n",
      "epoch :  2130 loss :  0.0037870127707719803\n",
      "epoch :  2131 loss :  0.0037855547852814198\n",
      "epoch :  2132 loss :  0.0037841976154595613\n",
      "epoch :  2133 loss :  0.0037827268242836\n",
      "epoch :  2134 loss :  0.003781334962695837\n",
      "epoch :  2135 loss :  0.0037799477577209473\n",
      "epoch :  2136 loss :  0.003778478130698204\n",
      "epoch :  2137 loss :  0.003777121426537633\n",
      "epoch :  2138 loss :  0.003775735152885318\n",
      "epoch :  2139 loss :  0.0037742741405963898\n",
      "epoch :  2140 loss :  0.003772880882024765\n",
      "epoch :  2141 loss :  0.0037714806385338306\n",
      "epoch :  2142 loss :  0.003770072478801012\n",
      "epoch :  2143 loss :  0.00376865710131824\n",
      "epoch :  2144 loss :  0.003767290385439992\n",
      "epoch :  2145 loss :  0.003765891073271632\n",
      "epoch :  2146 loss :  0.0037644277326762676\n",
      "epoch :  2147 loss :  0.003763092216104269\n",
      "epoch :  2148 loss :  0.003761670785024762\n",
      "epoch :  2149 loss :  0.003760209772735834\n",
      "epoch :  2150 loss :  0.003758904291316867\n",
      "epoch :  2151 loss :  0.0037574893794953823\n",
      "epoch :  2152 loss :  0.0037560747005045414\n",
      "epoch :  2153 loss :  0.0037547009997069836\n",
      "epoch :  2154 loss :  0.0037532784044742584\n",
      "epoch :  2155 loss :  0.0037518925964832306\n",
      "epoch :  2156 loss :  0.0037504720967262983\n",
      "epoch :  2157 loss :  0.003749097464606166\n",
      "epoch :  2158 loss :  0.00374772259965539\n",
      "epoch :  2159 loss :  0.0037462979089468718\n",
      "epoch :  2160 loss :  0.0037449661176651716\n",
      "epoch :  2161 loss :  0.00374354375526309\n",
      "epoch :  2162 loss :  0.0037421025335788727\n",
      "epoch :  2163 loss :  0.003740796120837331\n",
      "epoch :  2164 loss :  0.003739376086741686\n",
      "epoch :  2165 loss :  0.003737998427823186\n",
      "epoch :  2166 loss :  0.0037366386968642473\n",
      "epoch :  2167 loss :  0.00373521214351058\n",
      "epoch :  2168 loss :  0.0037338617257773876\n",
      "epoch :  2169 loss :  0.0037324470467865467\n",
      "epoch :  2170 loss :  0.0037310936022549868\n",
      "epoch :  2171 loss :  0.003729733172804117\n",
      "epoch :  2172 loss :  0.003728289622813463\n",
      "epoch :  2173 loss :  0.0037269904278218746\n",
      "epoch :  2174 loss :  0.0037255603820085526\n",
      "epoch :  2175 loss :  0.003724151523783803\n",
      "epoch :  2176 loss :  0.0037228744477033615\n",
      "epoch :  2177 loss :  0.003721428569406271\n",
      "epoch :  2178 loss :  0.003720048349350691\n",
      "epoch :  2179 loss :  0.003718749387189746\n",
      "epoch :  2180 loss :  0.003717313287779689\n",
      "epoch :  2181 loss :  0.003715960308909416\n",
      "epoch :  2182 loss :  0.003714571241289377\n",
      "epoch :  2183 loss :  0.0037132110446691513\n",
      "epoch :  2184 loss :  0.00371183711104095\n",
      "epoch :  2185 loss :  0.0037104343064129353\n",
      "epoch :  2186 loss :  0.0037091728299856186\n",
      "epoch :  2187 loss :  0.0037077448796480894\n",
      "epoch :  2188 loss :  0.0037063423078507185\n",
      "epoch :  2189 loss :  0.0037050482351332903\n",
      "epoch :  2190 loss :  0.0037036160938441753\n",
      "epoch :  2191 loss :  0.003702264279127121\n",
      "epoch :  2192 loss :  0.003700945060700178\n",
      "epoch :  2193 loss :  0.003699564840644598\n",
      "epoch :  2194 loss :  0.003698222804814577\n",
      "epoch :  2195 loss :  0.003696833271533251\n",
      "epoch :  2196 loss :  0.003695491701364517\n",
      "epoch :  2197 loss :  0.0036941072903573513\n",
      "epoch :  2198 loss :  0.0036927289329469204\n",
      "epoch :  2199 loss :  0.003691439051181078\n",
      "epoch :  2200 loss :  0.0036900516133755445\n",
      "epoch :  2201 loss :  0.003688690485432744\n",
      "epoch :  2202 loss :  0.0036873724311590195\n",
      "epoch :  2203 loss :  0.0036859516985714436\n",
      "epoch :  2204 loss :  0.003684647846966982\n",
      "epoch :  2205 loss :  0.003683301154524088\n",
      "epoch :  2206 loss :  0.0036819404922425747\n",
      "epoch :  2207 loss :  0.003680608468130231\n",
      "epoch :  2208 loss :  0.00367925176396966\n",
      "epoch :  2209 loss :  0.003677916247397661\n",
      "epoch :  2210 loss :  0.0036765248514711857\n",
      "epoch :  2211 loss :  0.0036751951556652784\n",
      "epoch :  2212 loss :  0.0036738752387464046\n",
      "epoch :  2213 loss :  0.0036724843084812164\n",
      "epoch :  2214 loss :  0.003671181621029973\n",
      "epoch :  2215 loss :  0.00366983818821609\n",
      "epoch :  2216 loss :  0.003668454010039568\n",
      "epoch :  2217 loss :  0.003667148295789957\n",
      "epoch :  2218 loss :  0.003665800439193845\n",
      "epoch :  2219 loss :  0.003664446994662285\n",
      "epoch :  2220 loss :  0.0036631268449127674\n",
      "epoch :  2221 loss :  0.003661800641566515\n",
      "epoch :  2222 loss :  0.0036604732740670443\n",
      "epoch :  2223 loss :  0.003659084439277649\n",
      "epoch :  2224 loss :  0.003657819237560034\n",
      "epoch :  2225 loss :  0.0036564450711011887\n",
      "epoch :  2226 loss :  0.0036550569348037243\n",
      "epoch :  2227 loss :  0.0036538199055939913\n",
      "epoch :  2228 loss :  0.003652464598417282\n",
      "epoch :  2229 loss :  0.0036511037033051252\n",
      "epoch :  2230 loss :  0.003649810329079628\n",
      "epoch :  2231 loss :  0.0036484473384916782\n",
      "epoch :  2232 loss :  0.0036471174098551273\n",
      "epoch :  2233 loss :  0.0036457807291299105\n",
      "epoch :  2234 loss :  0.0036444657016545534\n",
      "epoch :  2235 loss :  0.0036431713961064816\n",
      "epoch :  2236 loss :  0.0036418128293007612\n",
      "epoch :  2237 loss :  0.003640512702986598\n",
      "epoch :  2238 loss :  0.003639157861471176\n",
      "epoch :  2239 loss :  0.003637785091996193\n",
      "epoch :  2240 loss :  0.0036365576088428497\n",
      "epoch :  2241 loss :  0.0036352064926177263\n",
      "epoch :  2242 loss :  0.0036338604986667633\n",
      "epoch :  2243 loss :  0.003632591338828206\n",
      "epoch :  2244 loss :  0.0036312181036919355\n",
      "epoch :  2245 loss :  0.003629928920418024\n",
      "epoch :  2246 loss :  0.00362861598841846\n",
      "epoch :  2247 loss :  0.003627268597483635\n",
      "epoch :  2248 loss :  0.0036259996704757214\n",
      "epoch :  2249 loss :  0.003624632256105542\n",
      "epoch :  2250 loss :  0.0036233733408153057\n",
      "epoch :  2251 loss :  0.0036220080219209194\n",
      "epoch :  2252 loss :  0.0036206827498972416\n",
      "epoch :  2253 loss :  0.0036194417625665665\n",
      "epoch :  2254 loss :  0.0036180601455271244\n",
      "epoch :  2255 loss :  0.003616784466430545\n",
      "epoch :  2256 loss :  0.0036154899280518293\n",
      "epoch :  2257 loss :  0.0036141425371170044\n",
      "epoch :  2258 loss :  0.003612850559875369\n",
      "epoch :  2259 loss :  0.0036115478724241257\n",
      "epoch :  2260 loss :  0.0036102314479649067\n",
      "epoch :  2261 loss :  0.0036089285276830196\n",
      "epoch :  2262 loss :  0.0036076088435947895\n",
      "epoch :  2263 loss :  0.0036063771694898605\n",
      "epoch :  2264 loss :  0.0036050151102244854\n",
      "epoch :  2265 loss :  0.003603696823120117\n",
      "epoch :  2266 loss :  0.0036024407017976046\n",
      "epoch :  2267 loss :  0.003601075615733862\n",
      "epoch :  2268 loss :  0.0035998050589114428\n",
      "epoch :  2269 loss :  0.0035985433496534824\n",
      "epoch :  2270 loss :  0.0035972229670733213\n",
      "epoch :  2271 loss :  0.0035959365777671337\n",
      "epoch :  2272 loss :  0.0035946324933320284\n",
      "epoch :  2273 loss :  0.0035933293402194977\n",
      "epoch :  2274 loss :  0.0035920075606554747\n",
      "epoch :  2275 loss :  0.003590718610212207\n",
      "epoch :  2276 loss :  0.003589501604437828\n",
      "epoch :  2277 loss :  0.0035881325602531433\n",
      "epoch :  2278 loss :  0.0035868629347532988\n",
      "epoch :  2279 loss :  0.0035855756141245365\n",
      "epoch :  2280 loss :  0.0035842314828187227\n",
      "epoch :  2281 loss :  0.003582997713238001\n",
      "epoch :  2282 loss :  0.003581709461286664\n",
      "epoch :  2283 loss :  0.0035804063081741333\n",
      "epoch :  2284 loss :  0.0035791390109807253\n",
      "epoch :  2285 loss :  0.0035778270103037357\n",
      "epoch :  2286 loss :  0.0035765659995377064\n",
      "epoch :  2287 loss :  0.0035752623807638884\n",
      "epoch :  2288 loss :  0.003573975758627057\n",
      "epoch :  2289 loss :  0.0035727028734982014\n",
      "epoch :  2290 loss :  0.003571410896256566\n",
      "epoch :  2291 loss :  0.0035701740998774767\n",
      "epoch :  2292 loss :  0.003568848129361868\n",
      "epoch :  2293 loss :  0.003567542415112257\n",
      "epoch :  2294 loss :  0.0035663223825395107\n",
      "epoch :  2295 loss :  0.00356500712223351\n",
      "epoch :  2296 loss :  0.0035637239925563335\n",
      "epoch :  2297 loss :  0.0035624834708869457\n",
      "epoch :  2298 loss :  0.003561172168701887\n",
      "epoch :  2299 loss :  0.0035599120892584324\n",
      "epoch :  2300 loss :  0.0035586184822022915\n",
      "epoch :  2301 loss :  0.0035573597997426987\n",
      "epoch :  2302 loss :  0.0035560682881623507\n",
      "epoch :  2303 loss :  0.0035547432489693165\n",
      "epoch :  2304 loss :  0.003553596092388034\n",
      "epoch :  2305 loss :  0.0035522724501788616\n",
      "epoch :  2306 loss :  0.0035509620793163776\n",
      "epoch :  2307 loss :  0.003549759043380618\n",
      "epoch :  2308 loss :  0.003548432607203722\n",
      "epoch :  2309 loss :  0.0035471785813570023\n",
      "epoch :  2310 loss :  0.003545944346114993\n",
      "epoch :  2311 loss :  0.003544666338711977\n",
      "epoch :  2312 loss :  0.0035433978773653507\n",
      "epoch :  2313 loss :  0.003542104037478566\n",
      "epoch :  2314 loss :  0.0035408739931881428\n",
      "epoch :  2315 loss :  0.003539568977430463\n",
      "epoch :  2316 loss :  0.0035382970236241817\n",
      "epoch :  2317 loss :  0.0035371198318898678\n",
      "epoch :  2318 loss :  0.0035358064342290163\n",
      "epoch :  2319 loss :  0.003534534480422735\n",
      "epoch :  2320 loss :  0.003533299546688795\n",
      "epoch :  2321 loss :  0.0035319991875439882\n",
      "epoch :  2322 loss :  0.0035307605285197496\n",
      "epoch :  2323 loss :  0.0035295162815600634\n",
      "epoch :  2324 loss :  0.003528246656060219\n",
      "epoch :  2325 loss :  0.003526997985318303\n",
      "epoch :  2326 loss :  0.003525723470374942\n",
      "epoch :  2327 loss :  0.0035245264880359173\n",
      "epoch :  2328 loss :  0.0035232198424637318\n",
      "epoch :  2329 loss :  0.0035219595301896334\n",
      "epoch :  2330 loss :  0.00352074159309268\n",
      "epoch :  2331 loss :  0.0035194470547139645\n",
      "epoch :  2332 loss :  0.003518254728987813\n",
      "epoch :  2333 loss :  0.003516975324600935\n",
      "epoch :  2334 loss :  0.0035157236270606518\n",
      "epoch :  2335 loss :  0.003514496609568596\n",
      "epoch :  2336 loss :  0.0035132276825606823\n",
      "epoch :  2337 loss :  0.003511980175971985\n",
      "epoch :  2338 loss :  0.003510738257318735\n",
      "epoch :  2339 loss :  0.0035095077473670244\n",
      "epoch :  2340 loss :  0.0035082832910120487\n",
      "epoch :  2341 loss :  0.003506976645439863\n",
      "epoch :  2342 loss :  0.0035057836212217808\n",
      "epoch :  2343 loss :  0.003504518885165453\n",
      "epoch :  2344 loss :  0.0035032187588512897\n",
      "epoch :  2345 loss :  0.003502060193568468\n",
      "epoch :  2346 loss :  0.0035008019767701626\n",
      "epoch :  2347 loss :  0.0034995402675122023\n",
      "epoch :  2348 loss :  0.003498334903270006\n",
      "epoch :  2349 loss :  0.0034970573615282774\n",
      "epoch :  2350 loss :  0.003495850833132863\n",
      "epoch :  2351 loss :  0.003494605887681246\n",
      "epoch :  2352 loss :  0.0034933805000036955\n",
      "epoch :  2353 loss :  0.0034921527840197086\n",
      "epoch :  2354 loss :  0.0034908554516732693\n",
      "epoch :  2355 loss :  0.003489698749035597\n",
      "epoch :  2356 loss :  0.0034884330816566944\n",
      "epoch :  2357 loss :  0.0034871671814471483\n",
      "epoch :  2358 loss :  0.0034860013984143734\n",
      "epoch :  2359 loss :  0.0034847490023821592\n",
      "epoch :  2360 loss :  0.0034835021942853928\n",
      "epoch :  2361 loss :  0.003482300089672208\n",
      "epoch :  2362 loss :  0.0034810330253094435\n",
      "epoch :  2363 loss :  0.0034798395354300737\n",
      "epoch :  2364 loss :  0.003478575963526964\n",
      "epoch :  2365 loss :  0.003477365244179964\n",
      "epoch :  2366 loss :  0.0034761596471071243\n",
      "epoch :  2367 loss :  0.0034748739562928677\n",
      "epoch :  2368 loss :  0.003473730059340596\n",
      "epoch :  2369 loss :  0.0034724525175988674\n",
      "epoch :  2370 loss :  0.0034712112974375486\n",
      "epoch :  2371 loss :  0.0034700396936386824\n",
      "epoch :  2372 loss :  0.003468776121735573\n",
      "epoch :  2373 loss :  0.003467598930001259\n",
      "epoch :  2374 loss :  0.0034663905389606953\n",
      "epoch :  2375 loss :  0.003465130226686597\n",
      "epoch :  2376 loss :  0.0034639264922589064\n",
      "epoch :  2377 loss :  0.003462685737758875\n",
      "epoch :  2378 loss :  0.0034614980686455965\n",
      "epoch :  2379 loss :  0.0034602428786456585\n",
      "epoch :  2380 loss :  0.0034590568393468857\n",
      "epoch :  2381 loss :  0.0034578810445964336\n",
      "epoch :  2382 loss :  0.0034566130489110947\n",
      "epoch :  2383 loss :  0.003455408615991473\n",
      "epoch :  2384 loss :  0.003454220714047551\n",
      "epoch :  2385 loss :  0.0034529611002653837\n",
      "epoch :  2386 loss :  0.003451802534982562\n",
      "epoch :  2387 loss :  0.0034506015945225954\n",
      "epoch :  2388 loss :  0.0034493575803935528\n",
      "epoch :  2389 loss :  0.0034481477923691273\n",
      "epoch :  2390 loss :  0.0034469354432076216\n",
      "epoch :  2391 loss :  0.0034457470756024122\n",
      "epoch :  2392 loss :  0.0034444902557879686\n",
      "epoch :  2393 loss :  0.0034433440305292606\n",
      "epoch :  2394 loss :  0.003442138899117708\n",
      "epoch :  2395 loss :  0.0034408762585371733\n",
      "epoch :  2396 loss :  0.0034397419076412916\n",
      "epoch :  2397 loss :  0.003438520012423396\n",
      "epoch :  2398 loss :  0.0034372699446976185\n",
      "epoch :  2399 loss :  0.0034361169673502445\n",
      "epoch :  2400 loss :  0.003434881567955017\n",
      "epoch :  2401 loss :  0.003433726029470563\n",
      "epoch :  2402 loss :  0.0034325174055993557\n",
      "epoch :  2403 loss :  0.0034312948118895292\n",
      "epoch :  2404 loss :  0.0034301162231713533\n",
      "epoch :  2405 loss :  0.003428859170526266\n",
      "epoch :  2406 loss :  0.0034277259837836027\n",
      "epoch :  2407 loss :  0.0034265245776623487\n",
      "epoch :  2408 loss :  0.003425294067710638\n",
      "epoch :  2409 loss :  0.0034241769462823868\n",
      "epoch :  2410 loss :  0.0034229233860969543\n",
      "epoch :  2411 loss :  0.0034217126667499542\n",
      "epoch :  2412 loss :  0.0034205547999590635\n",
      "epoch :  2413 loss :  0.0034193117171525955\n",
      "epoch :  2414 loss :  0.0034181575756520033\n",
      "epoch :  2415 loss :  0.00341695174574852\n",
      "epoch :  2416 loss :  0.003415782004594803\n",
      "epoch :  2417 loss :  0.003414589911699295\n",
      "epoch :  2418 loss :  0.0034133470617234707\n",
      "epoch :  2419 loss :  0.0034122478682547808\n",
      "epoch :  2420 loss :  0.0034110143315047026\n",
      "epoch :  2421 loss :  0.0034098029136657715\n",
      "epoch :  2422 loss :  0.003408698830753565\n",
      "epoch :  2423 loss :  0.003407444339245558\n",
      "epoch :  2424 loss :  0.0034062806516885757\n",
      "epoch :  2425 loss :  0.003405132098123431\n",
      "epoch :  2426 loss :  0.0034038827288895845\n",
      "epoch :  2427 loss :  0.0034027330111712217\n",
      "epoch :  2428 loss :  0.003401548368856311\n",
      "epoch :  2429 loss :  0.0034003914333879948\n",
      "epoch :  2430 loss :  0.003399182576686144\n",
      "epoch :  2431 loss :  0.003397991182282567\n",
      "epoch :  2432 loss :  0.003396864514797926\n",
      "epoch :  2433 loss :  0.003395630046725273\n",
      "epoch :  2434 loss :  0.0033944572787731886\n",
      "epoch :  2435 loss :  0.0033933259546756744\n",
      "epoch :  2436 loss :  0.003392090555280447\n",
      "epoch :  2437 loss :  0.0033909857738763094\n",
      "epoch :  2438 loss :  0.003389788093045354\n",
      "epoch :  2439 loss :  0.003388579236343503\n",
      "epoch :  2440 loss :  0.0033874239306896925\n",
      "epoch :  2441 loss :  0.0033862325362861156\n",
      "epoch :  2442 loss :  0.0033851254265755415\n",
      "epoch :  2443 loss :  0.0033839046955108643\n",
      "epoch :  2444 loss :  0.00338273448869586\n",
      "epoch :  2445 loss :  0.003381602466106415\n",
      "epoch :  2446 loss :  0.0033803549595177174\n",
      "epoch :  2447 loss :  0.003379254136234522\n",
      "epoch :  2448 loss :  0.0033780746161937714\n",
      "epoch :  2449 loss :  0.0033768825232982635\n",
      "epoch :  2450 loss :  0.00337577098980546\n",
      "epoch :  2451 loss :  0.0033745572436600924\n",
      "epoch :  2452 loss :  0.003373392391949892\n",
      "epoch :  2453 loss :  0.0033722135704010725\n",
      "epoch :  2454 loss :  0.0033710666466504335\n",
      "epoch :  2455 loss :  0.003369924146682024\n",
      "epoch :  2456 loss :  0.003368732985109091\n",
      "epoch :  2457 loss :  0.003367601428180933\n",
      "epoch :  2458 loss :  0.003366426331922412\n",
      "epoch :  2459 loss :  0.0033652200363576412\n",
      "epoch :  2460 loss :  0.003364158095791936\n",
      "epoch :  2461 loss :  0.0033629320096224546\n",
      "epoch :  2462 loss :  0.0033617683220654726\n",
      "epoch :  2463 loss :  0.0033606491051614285\n",
      "epoch :  2464 loss :  0.0033594504930078983\n",
      "epoch :  2465 loss :  0.0033583238255232573\n",
      "epoch :  2466 loss :  0.0033571410458534956\n",
      "epoch :  2467 loss :  0.0033559955190867186\n",
      "epoch :  2468 loss :  0.0033548488281667233\n",
      "epoch :  2469 loss :  0.0033536460250616074\n",
      "epoch :  2470 loss :  0.0033525831531733274\n",
      "epoch :  2471 loss :  0.003351398976519704\n",
      "epoch :  2472 loss :  0.0033502113074064255\n",
      "epoch :  2473 loss :  0.0033491302747279406\n",
      "epoch :  2474 loss :  0.0033479039557278156\n",
      "epoch :  2475 loss :  0.0033467859029769897\n",
      "epoch :  2476 loss :  0.003345675300806761\n",
      "epoch :  2477 loss :  0.0033444836735725403\n",
      "epoch :  2478 loss :  0.003343357937410474\n",
      "epoch :  2479 loss :  0.0033421844709664583\n",
      "epoch :  2480 loss :  0.0033410564064979553\n",
      "epoch :  2481 loss :  0.0033398871310055256\n",
      "epoch :  2482 loss :  0.0033387341536581516\n",
      "epoch :  2483 loss :  0.0033376789651811123\n",
      "epoch :  2484 loss :  0.003336457535624504\n",
      "epoch :  2485 loss :  0.003335336921736598\n",
      "epoch :  2486 loss :  0.003334206761792302\n",
      "epoch :  2487 loss :  0.0033330023288726807\n",
      "epoch :  2488 loss :  0.0033319401554763317\n",
      "epoch :  2489 loss :  0.003330764826387167\n",
      "epoch :  2490 loss :  0.0033296081237494946\n",
      "epoch :  2491 loss :  0.003328504506498575\n",
      "epoch :  2492 loss :  0.0033273324370384216\n",
      "epoch :  2493 loss :  0.003326248377561569\n",
      "epoch :  2494 loss :  0.0033250383567065\n",
      "epoch :  2495 loss :  0.003323931712657213\n",
      "epoch :  2496 loss :  0.003322821808978915\n",
      "epoch :  2497 loss :  0.0033216297160834074\n",
      "epoch :  2498 loss :  0.003320583375170827\n",
      "epoch :  2499 loss :  0.00331941363401711\n",
      "epoch :  2500 loss :  0.0033182301558554173\n"
     ]
    }
   ],
   "source": [
    "for i in range(2500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "\n",
    "    print('epoch : ', (i + 1), \"loss : \", loss.item())\n",
    "    loss.backward() # 역전파 적용\n",
    "\n",
    "    optimizer.step() # 모든 파라미터 최신화\n",
    "\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.nn : neural network를 만드는데 꼭 필요한 모듈이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 8., 16., 24.])\n"
     ]
    }
   ],
   "source": [
    "class EgLayer(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.param\n",
    "\n",
    "myOb = EgLayer(8)\n",
    "output = myOb(torch.Tensor([1, 2, 3]))\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out = 6, 4, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-1.0388, -1.4482,  0.0329, -0.2891,  2.3674, -1.4050]]),\n",
       " tensor([[-1.1167]]))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "x = torch.randn((1, n_in))\n",
    "y = torch.randn((1, n_out))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-1.3527, -0.0236,  0.1597,  1.1823]]), tensor([[0.9111]]))"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "b1 = torch.randn((1, n_h))\n",
    "b2 = torch.randn((1, n_out))\n",
    "b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.6835,  1.1333,  0.9484,  0.5462],\n",
       "         [ 0.6131, -0.4834,  1.2760,  0.6268],\n",
       "         [ 0.0481, -0.2843,  1.0128,  0.1890],\n",
       "         [-0.0193,  1.0166, -0.0788, -0.8266],\n",
       "         [ 0.3201,  0.8299,  1.1345, -1.1021],\n",
       "         [-0.5183, -0.5655, -1.4727, -0.9215]]),\n",
       " tensor([[ 0.6082],\n",
       "         [-2.8289],\n",
       "         [-1.2786],\n",
       "         [-0.9487]]))"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "w1 = torch.randn(n_in, n_h)\n",
    "w2 = torch.randn(n_h, n_out)\n",
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1 / (1+torch.exp(-z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.mm(x, w1) + b1\n",
    "a1 = sigmoid_activation(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = torch.mm(a1, w2) + b2\n",
    "output = sigmoid_activation(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y - output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_delta(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_out = sigmoid_delta(output)\n",
    "delta_h = sigmoid_delta(a1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_outp = loss * delta_out\n",
    "loss_h = torch.mm(d_outp, w2.t())\n",
    "d_hidn = loss_h * delta_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}