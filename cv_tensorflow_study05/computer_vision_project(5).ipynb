{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('week09_venv': venv)"
  },
  "interpreter": {
   "hash": "e5245979aa73c3b83432dc2a37ab2e9505e3421d94f01464585bd10ff7b289bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### computer_vision_project.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "source": [
    "## TensorFlow Datasets\n",
    "### 사전에 만들어 놓은 머신러닝 데이터셋을 모아둔 것\n",
    "\n",
    "* 텐서로 이미 데이터를 준비\n",
    "* 잘 준비된 데이터 정리된 데이터로 실습을 할 수 있다\n",
    "* 여러가지 데이터 불러오는 방법을 실습할 수 있다.\n",
    "* 텐서플로 버전이 바뀌면서 새롭게 생기거나 변경된 경우에 빠르게 이 데이터셋으로 테스트를 할 수 있다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dataset_list = tfds.list_builders()\n",
    "print(\"food101\" in dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_data, test_data), dataset_info = tfds.load(\n",
    "    name=\"food101\", # 우리가 사용할 데이터셋 지정\n",
    "    split=[\"train\", \"validation\"],  # 데이터셋 나누는 부분\n",
    "    shuffle_files=True,  # 파일들을 섞어 줄것인지 default = False\n",
    "    as_supervised=True, # tuple 형태 (data, label) 로 데이터를 다운로드 할 것인지 여부 / False면 dictionary 형태\n",
    "    with_info=True, # 데이터셋의 메타 정보도 다운로드 (라벨, 샘플의 갯수 등등)\n",
    "    download=False)"
   ]
  },
  {
   "source": [
    "### Windows의 경우 C:\\Users\\[자신의 아이디]\\tensorflow_datasets 에 다운로드 한 후 압축 해제"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=101),\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "dataset_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['apple_pie',\n",
       " 'baby_back_ribs',\n",
       " 'baklava',\n",
       " 'beef_carpaccio',\n",
       " 'beef_tartare',\n",
       " 'beet_salad',\n",
       " 'beignets',\n",
       " 'bibimbap',\n",
       " 'bread_pudding',\n",
       " 'breakfast_burrito',\n",
       " 'bruschetta',\n",
       " 'caesar_salad',\n",
       " 'cannoli',\n",
       " 'caprese_salad',\n",
       " 'carrot_cake',\n",
       " 'ceviche',\n",
       " 'cheesecake',\n",
       " 'cheese_plate',\n",
       " 'chicken_curry',\n",
       " 'chicken_quesadilla']"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "class_names = dataset_info.features[\"label\"].names\n",
    "class_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_sample = train_data.take(1) # (image_tensor, label) 왜? as_supervised=True 로 설정을 했기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "train_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TakeDataset"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "type(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Image shape : (512, 512, 3)\nImage dtype : <dtype: 'uint8'>\nTarget class : 31\nClass name : donuts\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_one_sample:\n",
    "    print(f\"Image shape : {image.shape}\")\n",
    "    print(f\"Image dtype : {image.dtype}\")\n",
    "    print(f\"Target class : {label}\")\n",
    "    print(f\"Class name : {class_names[label.numpy()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy=\n",
       "array([[[131,  86,  55],\n",
       "        [134,  89,  58],\n",
       "        [137,  92,  61],\n",
       "        ...,\n",
       "        [131,  44,   0],\n",
       "        [135,  49,   2],\n",
       "        [138,  52,   5]],\n",
       "\n",
       "       [[134,  89,  58],\n",
       "        [135,  90,  59],\n",
       "        [135,  90,  59],\n",
       "        ...,\n",
       "        [133,  46,   1],\n",
       "        [134,  48,   1],\n",
       "        [134,  48,   1]],\n",
       "\n",
       "       [[136,  91,  60],\n",
       "        [137,  92,  61],\n",
       "        [139,  94,  63],\n",
       "        ...,\n",
       "        [137,  53,   7],\n",
       "        [135,  51,   5],\n",
       "        [131,  47,   1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[130,  23,   3],\n",
       "        [134,  31,  14],\n",
       "        [132,  36,  24],\n",
       "        ...,\n",
       "        [243, 248, 242],\n",
       "        [243, 248, 242],\n",
       "        [243, 248, 242]],\n",
       "\n",
       "       [[128,  22,   0],\n",
       "        [129,  26,   7],\n",
       "        [124,  28,  14],\n",
       "        ...,\n",
       "        [243, 248, 241],\n",
       "        [242, 247, 240],\n",
       "        [242, 247, 240]],\n",
       "\n",
       "       [[132,  26,   2],\n",
       "        [133,  31,   9],\n",
       "        [126,  29,  13],\n",
       "        ...,\n",
       "        [242, 247, 240],\n",
       "        [242, 247, 240],\n",
       "        [242, 247, 240]]], dtype=uint8)>"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "image.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=uint8, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=uint8, numpy=255>)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tf.reduce_min(image), tf.reduce_max(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(-0.5, 511.5, 511.5, -0.5)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.title(class_names[label.numpy()])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(image, label, img_shape = 224):\n",
    "    image = tf.image.resize(image, [img_shape, img_shape])\n",
    "    return tf.cast(image, tf.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_img = preprocess_img(image, label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[131  86  55]\n  [134  89  58]\n  [137  92  61]\n  ...\n  [131  44   0]\n  [135  49   2]\n  [138  52   5]]\n\n [[134  89  58]\n  [135  90  59]\n  [135  90  59]\n  ...\n  [133  46   1]\n  [134  48   1]\n  [134  48   1]]] (512, 512, 3) <dtype: 'uint8'>\n[[[1.34030609e+02 8.90306168e+01 5.80306129e+01]\n  [1.36045914e+02 9.10459213e+01 6.00459175e+01]\n  [1.37846939e+02 9.28469391e+01 6.38469391e+01]\n  ...\n  [1.30265366e+02 4.79183846e+01 1.62753057e+00]\n  [1.30760223e+02 4.37602272e+01 4.59267795e-02]\n  [1.34739838e+02 4.87398338e+01 1.73983324e+00]]\n\n [[1.41551025e+02 9.65510254e+01 6.55510254e+01]\n  [1.49081650e+02 1.04081635e+02 7.30816345e+01]\n  [1.51632660e+02 1.06632652e+02 7.56326523e+01]\n  ...\n  [1.30357101e+02 4.85713654e+01 1.99989128e+00]\n  [1.34147995e+02 5.01479988e+01 4.14799976e+00]\n  [1.33571289e+02 4.95712891e+01 3.57128906e+00]]] (224, 224, 3) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{image[:2]} {image.shape} {image.dtype}\")\n",
    "print(f\"{preprocessed_img[:2]} {preprocessed_img.shape} {preprocessed_img.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(512, 512, 3)\n(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{image.shape}\")\n",
    "print(f\"{preprocessed_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<dtype: 'uint8'>\n<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{image.dtype}\")\n",
    "print(f\"{preprocessed_img.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(preprocessed_img/255.)\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "source": [
    "### 현재 우리가 가지고 있는 이미지 데이터는 총 101,000개 입니다. (학습용 및 테스트용 모두 합친 것)\n",
    "### batch를 잘 고려해야하는 상황! => 메모리를 효율적으로 사용할 수 있기 때문에!\n",
    "\n",
    "### 101,000개 이미지와 라벨 정보를 32개 이미지와 라벨 정보로 구분해서 처리\n",
    "\n",
    "### tf.data API의 메서드들을 사용!\n",
    "\n",
    "[tf data API](https://www.tensorflow.org/guide/data_performance)\n",
    "\n",
    "\n",
    "* map()\n",
    "* shuffler()\n",
    "* batch()\n",
    "* prefetch()\n",
    "* cache()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 배치로 처리할 때 고려해야 할 사항\n",
    "1. 다른 shape의 텐서들을 배치로 처리할 수 없습니다. 이미지 resize와 같은 것을 처리!\n",
    "\n",
    "2. shuffle()는 섞을 이미지의 수를 유지를 합니다. 이 때 shuffle의 대상은 학습 데이터의 모두 이미지가 대상\n",
    "\n",
    "   학습 데이터의 크기가 크다면, 메모리에 효율적 사용이 어렵다. 1,000이나 10,000로 사용하는 것도 나쁘지는 않다.\n",
    "\n",
    "\n",
    "3. num_parallel_calls 파라미터를 사용할 수 있는 메서드에서는 성능을 개선시킬 수 있습니다. num_parallel_calls = tf.data.AUTORUNE 설정!\n",
    "\n",
    "\n",
    "4. 데이터셋이 메모리게 잘 맞으면 cache()를 사용할 수 없습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![이미지](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-prefetching-from-hands-on-ml.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 일반적인 작업 순서\n",
    "origianl dataset -> map() -> shuffle() -> batch() -> prefetch() -> PrefetchDataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(\n",
    "    map_func = preprocess_img,\n",
    "    num_parallel_calls = tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "train_data = train_data.shuffle(buffer_size = 1000).batch(batch_size=32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_data = test_data.map(\n",
    "    map_func = preprocess_img,\n",
    "    num_parallel_calls = tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)>,\n",
       " <PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"model_checkpoint/cp.ckpt\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor = \"val_accuracy\",\n",
    "    save_best_only = True,\n",
    "    save_weight_only = True,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(policy = \"mixed_float16\")"
   ]
  },
  {
   "source": [
    "## mixed precision 학습\n",
    "### 일반적으로 tensorflow의 텐서는 기본적으로 float32 데이터형!\n",
    "### computer science에서는 float32를 single-precision floating-point 형식, 32비트를 컴퓨터 메모리에서 사용한다는 것을 의미\n",
    "### GPU에는 제한된 메모리를 가지고 있습니다. 그래서 GPU 동시에 float32 형식의 텐서들만을 처리할 수 있습니다.\n",
    "### Mixed Precision 학습은 제한된 메모리를 가진 GPU에서 메모리를 더 잘 활용하기 위해 float16, float32을 혼합해서 사용하는 방식입니다.\n",
    "### float16은 16비트로 숫자를 표현하는 것으로 float32의 메모리 사용의 반으로 숫자를 표현!\n",
    "### GPU를 활용해서 혼합 정밀도 학습을 사용하면 최대 3배까지 성능을 향상시킬 수 있다.\n",
    "### GPU 점수가 7.0 이상이어야 혼합 정밀도 학습이 작동합니다.\n",
    "\n",
    "[Mixed Precision 학습](https://www.tensorflow.org/guide/mixed_precision#supported_hardware)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(policy = \"float32\") # mixed_float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Policy \"float32\">"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "mixed_precision.global_policy()"
   ]
  },
  {
   "source": [
    "### Callback도 준비해 놓았고,\n",
    "### Mixed Precision도 설정해 놓았고,\n",
    "### 이제 모델을 만들면 됩니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 현재 우리 데이터셋은 아주 큽니다. food101은 용량이 5GB정도입니다.\n",
    "### 전이학습 (EfficientNetB0)으로 fine-tuning을 하려합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 일반적인 전이학습의 단계\n",
    "1. feature extraction 모델 구축 (사전 훈련한 모델의 상위 몇 개 레이어 교체)\n",
    "2. 하위 레이어가 고정된 epoch 동안 학습\n",
    "3. 필요한 경우 unfrozen으로 레이어의 상태를 바꾸어서 fine-tuning을 실행"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D(name = \"pooling_layer\")(x)\n",
    "x = layers.Dense(len(class_names))(x)\n",
    "\n",
    "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    loss = \"sparse_categorical_crossentropy\",  # 라벨의 상태가 one-hot encoding 상태가 sparse_categorical_corssentropy\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_layer (InputLayer)     [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n_________________________________________________________________\npooling_layer (GlobalAverage (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 101)               129381    \n_________________________________________________________________\nsoftmax_float32 (Activation) (None, 101)               0         \n=================================================================\nTotal params: 4,178,952\nTrainable params: 129,381\nNon-trainable params: 4,049,571\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_layer True float32 <Policy \"float32\">\nefficientnetb0 False float32 <Policy \"float32\">\npooling_layer True float32 <Policy \"float32\">\ndense True float32 <Policy \"float32\">\nsoftmax_float32 True float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_1 False float32 <Policy \"float32\">\nrescaling False float32 <Policy \"float32\">\nnormalization False float32 <Policy \"float32\">\nstem_conv_pad False float32 <Policy \"float32\">\nstem_conv False float32 <Policy \"float32\">\nstem_bn False float32 <Policy \"float32\">\nstem_activation False float32 <Policy \"float32\">\nblock1a_dwconv False float32 <Policy \"float32\">\nblock1a_bn False float32 <Policy \"float32\">\nblock1a_activation False float32 <Policy \"float32\">\nblock1a_se_squeeze False float32 <Policy \"float32\">\nblock1a_se_reshape False float32 <Policy \"float32\">\nblock1a_se_reduce False float32 <Policy \"float32\">\nblock1a_se_expand False float32 <Policy \"float32\">\nblock1a_se_excite False float32 <Policy \"float32\">\nblock1a_project_conv False float32 <Policy \"float32\">\nblock1a_project_bn False float32 <Policy \"float32\">\nblock2a_expand_conv False float32 <Policy \"float32\">\nblock2a_expand_bn False float32 <Policy \"float32\">\nblock2a_expand_activation False float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[1].layers[:20]:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TensorBoard 로그 파일을 저장한 디렉토리 : traning_logs/efficientnetb0_101_classes_all_data_feature_extract/20210706-121706\n",
      "Epoch 1/3\n",
      "2368/2368 [==============================] - 3738s 2s/step - loss: 1.8203 - accuracy: 0.5570 - val_loss: 1.2229 - val_accuracy: 0.6790\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.67903, saving model to model_checkpoint/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: model_checkpoint/cp.ckpt/assets\n",
      "INFO:tensorflow:Assets written to: model_checkpoint/cp.ckpt/assets\n",
      "Epoch 2/3\n",
      "2368/2368 [==============================] - 4542s 2s/step - loss: 1.2919 - accuracy: 0.6670 - val_loss: 1.1348 - val_accuracy: 0.6997\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.67903 to 0.69968, saving model to model_checkpoint/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: model_checkpoint/cp.ckpt/assets\n",
      "INFO:tensorflow:Assets written to: model_checkpoint/cp.ckpt/assets\n",
      "Epoch 3/3\n",
      "2368/2368 [==============================] - 5626s 2s/step - loss: 1.1405 - accuracy: 0.7030 - val_loss: 1.0866 - val_accuracy: 0.7034\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.69968 to 0.70339, saving model to model_checkpoint/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: model_checkpoint/cp.ckpt/assets\n",
      "INFO:tensorflow:Assets written to: model_checkpoint/cp.ckpt/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_101_food_classes_feature_extract = model.fit(\n",
    "    train_data,\n",
    "    epochs = 3,\n",
    "    steps_per_epoch= len(train_data),\n",
    "    validation_data = test_data,\n",
    "    validation_steps = int(0.15 * len(test_data)),\n",
    "    callbacks = [\n",
    "        create_tensorboard_callback(\"traning_logs\", \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
    "        model_checkpoint\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "790/790 [==============================] - 1290s 2s/step - loss: 1.0811 - accuracy: 0.7072\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.081135630607605, 0.7071683406829834]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "results_feature_extract_model = model.evaluate(test_data)\n",
    "results_feature_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 잘 만든 모델이 있다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_layer (InputLayer)     [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n_________________________________________________________________\npooling_layer (GlobalAverage (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 101)               129381    \n_________________________________________________________________\nsoftmax_float32 (Activation) (None, 101)               0         \n=================================================================\nTotal params: 4,178,952\nTrainable params: 129,381\nNon-trainable params: 4,049,571\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cloned_model = tf.keras.models.clone_model(model)\n",
    "cloned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model_checkpoint/cp.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbd5478e790>"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "cloned_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_model.compile(\n",
    "    loss = \"sparse_categorical_crossentropy\",  # 라벨의 상태가 one-hot encoding 상태가 sparse_categorical_corssentropy\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "790/790 [==============================] - 1171s 1s/step - loss: 1.0811 - accuracy: 0.7072\n"
     ]
    }
   ],
   "source": [
    "results_feature_extract_model_with_loaded_weights = cloned_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "assert np.isclose(\n",
    "    results_feature_extract_model,\n",
    "    results_feature_extract_model_with_loaded_weights    \n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_1 True float32 <Policy \"float32\">\nrescaling False float32 <Policy \"float32\">\nnormalization False float32 <Policy \"float32\">\nstem_conv_pad False float32 <Policy \"float32\">\nstem_conv False float32 <Policy \"float32\">\nstem_bn False float32 <Policy \"float32\">\nstem_activation False float32 <Policy \"float32\">\nblock1a_dwconv False float32 <Policy \"float32\">\nblock1a_bn False float32 <Policy \"float32\">\nblock1a_activation False float32 <Policy \"float32\">\nblock1a_se_squeeze False float32 <Policy \"float32\">\nblock1a_se_reshape False float32 <Policy \"float32\">\nblock1a_se_reduce False float32 <Policy \"float32\">\nblock1a_se_expand False float32 <Policy \"float32\">\nblock1a_se_excite False float32 <Policy \"float32\">\nblock1a_project_conv False float32 <Policy \"float32\">\nblock1a_project_bn False float32 <Policy \"float32\">\nblock2a_expand_conv False float32 <Policy \"float32\">\nblock2a_expand_bn False float32 <Policy \"float32\">\nblock2a_expand_activation False float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "for layer in cloned_model.layers[1].layers[:20]:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: efficientnetb0_feature_extract_model/assets\n",
      "INFO:tensorflow:Assets written to: efficientnetb0_feature_extract_model/assets\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"efficientnetb0_feature_extract_model\"\n",
    "model.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ion (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_203920) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_205627) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_204726) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_241233) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_239492) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_240135) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_241189) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_196694) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_204962) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_203945) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_204061) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_205782) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_240822) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_240745) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_238714) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_205142) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_238438) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_237263) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_240866) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_241600) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_203824) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_205592) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_236943) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_205863) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_241479) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_239004) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_204085) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_205177) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_236503) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_238317) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_204536) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_204667) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_203484) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_203624) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_205021) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_237630) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_204571) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_236744) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_238071) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_239125) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_205282) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_203684) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_204357) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_203649) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_205258) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_241846) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_238637) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_205317) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_204986) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_241967) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_233120) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_240425) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_237064) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_237340) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_203789) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_205437) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_231361) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_217648) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_237950) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_240502) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_204276) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_225596) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_241112) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_240179) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_205568) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_237020) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_237751) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_204381) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_241556) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_234744) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_204216) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_240546) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_239691) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_204831) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_236623) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_238394) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_204120) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_239371) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_204866) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_237384) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_204416) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_204241) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_204807) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_239812) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_213966) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_239768) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_236700) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_203765) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_203980) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_227229) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_205117) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_238027) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_204512) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_238758) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_205723) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_241923) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_240058) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_239081) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_242166) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_205747) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_203543) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_203508) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_205472) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_204691) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "# 저장한 모델을 가져와기\n",
    "# from keras.models import load_model\n",
    "\n",
    "new_model = tf.keras.models.load_model(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_layer (InputLayer)     [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n_________________________________________________________________\npooling_layer (GlobalAverage (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 101)               129381    \n_________________________________________________________________\nsoftmax_float32 (Activation) (None, 101)               0         \n=================================================================\nTotal params: 4,178,952\nTrainable params: 129,381\nNon-trainable params: 4,049,571\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype_policy를 유지하고 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 모델의 성능을 확인해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 모델의 성능이 이전 모델의 성능과 같은 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}