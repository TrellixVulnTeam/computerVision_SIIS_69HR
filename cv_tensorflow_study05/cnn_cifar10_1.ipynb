{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('week09_venv': venv)"
  },
  "interpreter": {
   "hash": "e5245979aa73c3b83432dc2a37ab2e9505e3421d94f01464585bd10ff7b289bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## cnn_cifar10.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision # tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root = \"./data\",\n",
    "    train = True,\n",
    "    download= True,\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "type(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "type(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader._SingleProcessDataLoaderIter"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "data_iter = iter(trainloader)\n",
    "type(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 32, 32])\n6\n"
     ]
    }
   ],
   "source": [
    "images, labels = data_iter.next()\n",
    "print(images.shape)\n",
    "print(labels.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1, 3, 32, 32)\n",
    "# 1 : Batch Size\n",
    "# 3 : channel in input\n",
    "# 32, 32 : image의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "classes[labels.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size = 4,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "data_iter = iter(trainloader)\n",
    "images, labels = data_iter.next()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7, 4, 5, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "image_data = images[0]\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "type(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.8549, 0.7647, 0.6745,  ..., 0.5529, 0.6235, 0.6118],\n",
       "         [0.6980, 0.6392, 0.5765,  ..., 0.5137, 0.6039, 0.6353],\n",
       "         [0.5569, 0.5961, 0.5137,  ..., 0.4275, 0.5647, 0.6314],\n",
       "         ...,\n",
       "         [0.5490, 0.5255, 0.4745,  ..., 0.3608, 0.3373, 0.5373],\n",
       "         [0.7137, 0.6824, 0.6706,  ..., 0.4471, 0.4863, 0.4667],\n",
       "         [0.7569, 0.7098, 0.7373,  ..., 0.6275, 0.5961, 0.4510]],\n",
       "\n",
       "        [[0.8784, 0.7922, 0.6784,  ..., 0.5569, 0.6314, 0.6235],\n",
       "         [0.7098, 0.6510, 0.5569,  ..., 0.5216, 0.6039, 0.6275],\n",
       "         [0.5569, 0.5882, 0.4627,  ..., 0.4392, 0.5569, 0.6039],\n",
       "         ...,\n",
       "         [0.4627, 0.4392, 0.3882,  ..., 0.3020, 0.2745, 0.4588],\n",
       "         [0.6157, 0.5843, 0.5725,  ..., 0.3843, 0.4196, 0.3882],\n",
       "         [0.6431, 0.5961, 0.6235,  ..., 0.5529, 0.5216, 0.3725]],\n",
       "\n",
       "        [[0.8706, 0.7804, 0.6667,  ..., 0.4745, 0.5490, 0.5686],\n",
       "         [0.6745, 0.6196, 0.5373,  ..., 0.4314, 0.5451, 0.5843],\n",
       "         [0.4980, 0.5333, 0.4431,  ..., 0.3451, 0.5137, 0.5725],\n",
       "         ...,\n",
       "         [0.4157, 0.3922, 0.3412,  ..., 0.2784, 0.2510, 0.4196],\n",
       "         [0.5569, 0.5255, 0.5137,  ..., 0.3569, 0.3922, 0.3490],\n",
       "         [0.5686, 0.5255, 0.5490,  ..., 0.5176, 0.4863, 0.3333]]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = image_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "np_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = np.transpose(np_image, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "np_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(np_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "horse\n"
     ]
    }
   ],
   "source": [
    "print(classes[labels[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(image_data, label):\n",
    "    np_image = image_data.numpy()\n",
    "    np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    plt.figure(figsize = (2, 2))\n",
    "    plt.imshow(np_image)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_show(images[1], classes[labels[1].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Convolution Layer\n",
    "class FirstCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 5, 3) # (입력 채널의 수, 출력 채널의 수, 커널 사이즈) kernel size 3 => (3, 3)의 matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = FirstCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cnn(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 5, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4, 10, 30, 30)\n",
    "# 4: 이미지의 갯수 batch_size\n",
    "# 10 : 출력 채널의 수\n",
    "# (30, 30) : 결과 이미지의 크기\n",
    "# default padding = (0, 0), stride = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 30, 30])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([30, 30])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "sample = out[0, 0, :, :]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = out[0, 1, :, :]\n",
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = out[0, 4, :, :]\n",
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3, \n",
    "            padding = (1, 1)\n",
    "        ) # (입력 채널의 수, 출력 채널의 수, 커널 사이즈) kernel size 3 => (3, 3)의 matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = FirstCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cnn(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 32, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "sample = out[0, 0, :, :]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3, \n",
    "            # padding = (1, 1),\n",
    "            stride = (2, 2)\n",
    "        ) # (입력 채널의 수, 출력 채널의 수, 커널 사이즈) kernel size 3 => (3, 3)의 matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 15, 15])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "cnn = FirstCNN()\n",
    "out = cnn(images)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 15])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "sample = out[0, 0, :, :]\n",
    "sample.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepCNN\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, 3),\n",
    "            nn.Conv2d(10, 5, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepCNN 클래스에서\n",
    "# 1st CNN Layer : Kernel size = (3, 3) | output 채널의 수 = 10 | padding (0, 0) | stride = (1, 1)\n",
    "# 2st CNN Layer : Kernel size = (3, 3) | output 채널의 수 = 5 | padding (0, 0) | stride = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 5, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "deep = DeepCNN()\n",
    "out = deep(images)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = out[0, 0, :, :]\n",
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoolingCNN\n",
    "class avg_pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(avg_pool, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, 3),\n",
    "            nn.Conv2d(10, 5, 3),\n",
    "            nn.AvgPool2d(2, stride = 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 5, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "avg = avg_pool()\n",
    "out = avg(images)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = out[0, 0, :, :]\n",
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pool CNN\n",
    "class max_pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(max_pool, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, 3),\n",
    "            nn.Conv2d(10, 5, 3),\n",
    "            nn.MaxPool2d(2, stride = 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 5, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "max_p = max_pool()\n",
    "out = max_p(images)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = out[0, 0, :, :]\n",
    "plt.figure(figsize = (2, 2))\n",
    "plt.imshow(sample.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-0.1973, -0.7061, -0.0542,  ..., -0.9220, -0.9616,  1.8430],\n",
       "         [ 1.8451, -0.1032, -1.1835,  ..., -2.5936, -0.9152, -0.6460],\n",
       "         [-0.1354,  0.1815, -1.4497,  ...,  0.5736,  0.6619,  2.0763],\n",
       "         [-0.1157,  0.8591,  0.7713,  ...,  2.1441,  1.1046,  1.2000],\n",
       "         [-0.9197, -0.2649, -0.6238,  ...,  0.0696, -1.0107, -1.1691]],\n",
       "\n",
       "        [[-0.6029,  1.6478, -1.4422,  ..., -0.5007,  0.9802, -1.2160],\n",
       "         [-0.8710, -1.3343,  0.5929,  ..., -1.5177,  1.5487,  1.7732],\n",
       "         [-1.0905,  0.4040,  1.0669,  ..., -1.0373, -0.0651,  1.0246],\n",
       "         [-0.4822,  0.7561,  0.6301,  ..., -0.6199,  0.4203, -0.9041],\n",
       "         [ 2.4305,  0.0772, -0.2567,  ...,  0.1744, -0.3325,  0.3803]],\n",
       "\n",
       "        [[-0.6330,  0.7665, -0.9902,  ..., -0.2284, -1.0260,  0.6020],\n",
       "         [ 0.7665, -0.2113, -0.4357,  ..., -0.5816,  0.9237,  0.1411],\n",
       "         [ 0.3198, -0.4730, -0.2941,  ...,  0.6804, -0.1432,  0.7041],\n",
       "         [-0.3422,  1.4484, -0.1046,  ..., -0.7255,  2.4459,  0.7640],\n",
       "         [-0.7199,  0.0173, -0.5019,  ...,  1.2638,  0.7739, -1.3810]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.9211,  0.4187,  0.2987,  ..., -0.6365,  0.3709, -0.9376],\n",
       "         [-0.4251,  0.5723,  0.9024,  ...,  0.3010,  0.6442, -0.3528],\n",
       "         [-0.3127, -0.3401, -0.1852,  ...,  0.7139, -1.8087,  1.0854],\n",
       "         [ 2.7111,  1.9914, -1.2444,  ...,  0.0878, -0.9767,  1.4190],\n",
       "         [ 1.8114, -1.1336, -0.2109,  ...,  0.2216,  0.6302,  0.2852]],\n",
       "\n",
       "        [[-1.4392,  2.4228, -0.7902,  ..., -0.8171,  1.5507, -1.4607],\n",
       "         [-0.0471,  0.4042, -0.4138,  ..., -0.1811,  0.3441, -1.3470],\n",
       "         [-0.3787,  0.7445, -0.6282,  ..., -1.0245, -0.1779, -0.0068],\n",
       "         [ 1.1314,  0.1893, -0.3241,  ..., -1.3547, -1.4265, -0.7988],\n",
       "         [-0.6679,  1.3967,  0.1848,  ...,  1.8506,  1.4748, -1.8992]],\n",
       "\n",
       "        [[-0.0724,  0.2699, -1.3511,  ...,  0.0248,  0.4177,  1.4428],\n",
       "         [ 0.2646, -0.4443, -1.3586,  ..., -0.6474,  0.4232,  1.4778],\n",
       "         [-0.4615, -0.0473,  0.0713,  ..., -0.1800, -0.5719,  1.4137],\n",
       "         [-1.3922,  0.2565,  1.0580,  ...,  2.5286,  0.3292, -0.2502],\n",
       "         [ 0.0391, -0.3922, -0.0199,  ...,  1.9645,  1.1969,  0.9848]]])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "x = torch.randn(500, 5, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([25000])"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "x = torch.flatten(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.1973, -0.7061, -0.0542,  ...,  1.9645,  1.1969,  0.9848])"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-0.1484, -1.2934,  0.4788,  ...,  2.0988,  0.4398, -0.8131],\n",
       "         [-0.5530,  0.3805, -0.8099,  ...,  1.3926,  0.8337,  0.0306],\n",
       "         [-0.6351,  0.3708, -0.0352,  ..., -0.4384, -0.1418,  1.9801],\n",
       "         ...,\n",
       "         [ 0.6648,  1.1565, -0.9017,  ..., -0.7421,  0.8320,  1.7531],\n",
       "         [-1.4427,  0.1126, -1.0231,  ..., -0.3694,  1.5076,  0.4769],\n",
       "         [ 0.6070, -0.9259,  0.2712,  ..., -0.5218, -2.6328, -1.4084]]),\n",
       " torch.Size([500, 50]))"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "x = torch.randn(500, 5, 10)\n",
    "x = torch.flatten(x, 1)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv2d(3,6,5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.dense_model = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, 10)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv_model(x)\n",
    "        y = torch.flatten(y, 1)\n",
    "        y = self.dense_model(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()\n",
    "out = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.1207, -0.1186,  0.0047, -0.0199, -0.1331,  0.0983, -0.1083,  0.0171,\n",
       "         -0.0047,  0.0509],\n",
       "        [-0.1390, -0.1154, -0.0036, -0.0250, -0.1225,  0.1047, -0.1141,  0.0036,\n",
       "         -0.0023,  0.0663],\n",
       "        [-0.1221, -0.1199,  0.0019, -0.0212, -0.1292,  0.1042, -0.1173,  0.0180,\n",
       "         -0.0133,  0.0624],\n",
       "        [-0.1410, -0.1179, -0.0007, -0.0196, -0.1263,  0.0921, -0.1106,  0.0162,\n",
       "          0.0100,  0.0559]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.1047, grad_fn=<MaxBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "torch.max(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.0983, 0.1047, 0.1042, 0.0921], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([5, 5, 5, 5]))"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "torch.max(out, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values, predict_class = torch.max(out, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0983, 0.1047, 0.1042, 0.0921], grad_fn=<MaxBackward0>),\n",
       " tensor([5, 5, 5, 5]))"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "max_values, predict_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}